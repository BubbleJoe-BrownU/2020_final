---
title: "project2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(dplyr)
library(ggplot2)
library(car)
library(olsrr)
library(pROC)
library(caret)
library(glmnet)
library(ROSE)
# for tree-based methods
library(rpart)
library(rpart.plot)
# for random forest models
library(randomForest)
```

```{r}
rm(list = ls())
data_wave_2 <- read_dta('dataset/FFdata/wave2/FF_wave2_2020v2.dta')
data_wave_1 <- read_dta('dataset/FFdata/wave1/FF_wave1_2020v2.dta')
```

```{r}

wave_2_filtered <- data_wave_2[, c("idnum", "cf2md_case_lib", "cf2age", "cm2fbir", "cf2edu", 
                          "cm2edu", "f2h7a", "m2c33", "cm2md_case_lib", "cf2marm", 
                          "cf2cohm", "f2b1", "f2b1a", "cf2povco")]
wave_1_filtered <- data_wave_1[, c("idnum", "m1h3", "m1h3a", "f1h3", "f1h3a")]

combined_data <- left_join(wave_2_filtered, wave_1_filtered, by = "idnum") 

# combine missing values in categorical columns into its own category
cols_to_encode = c()
combine_negatives <- function(x) {
  ifelse(x < 0, -1, x)
}
combined_data <- combined_data %>%
  mutate_at(c("cf2edu","cm2edu", "f2h7a", "m2c33", "cm2md_case_lib", "cf2marm", 
                  "cf2cohm", "f2b1", "f2b1a","m1h3", "m1h3a", "f1h3","f1h3a"), ~ combine_negatives(.))

# delete missing values for these three columns
combined_data_filtered <- combined_data %>%
  filter(cf2md_case_lib >= 0, cf2age >= 0, cm2fbir >= 0) %>%
  #change values for m2c33, changing from 201 to 1 (working), 202 to 2(unemployed), 203 to 3( in jail)
  mutate(m2c33 = case_when(
    m2c33 == 201 ~ 1,
    m2c33 == 202 ~ 2,
    m2c33 == 203 ~ 3,
    TRUE ~ m2c33
  ))

# cf2edu >= 0,
#          cm2edu >= 0, f2h7a >= 0, m2c33 >= 0, cm2md_case_lib >= 0, cf2marm >= 0,
#          cf2cohm >= 0, f2b1 >= 0, f2b1a >= 0, cf2povco >= 0,m1h3 >= 0,m1h3a >= 0,
#          f1h3 >= 0,f1h3a >= 0 )


# drop index column
combined_data_filtered <- combined_data_filtered[, !names(combined_data_filtered) == 'idnum']

columns = data.frame(colnames(combined_data_filtered))
```


```{r}
# summary(combined_data_filtered)
# table(combined_data_filtered$cf2md_case_lib)
```


```{r}
# check missing data
colMeans(combined_data_filtered < -1)
```
```{r}
# check datatypes of columns:
str(combined_data_filtered)
```
```{r}
# change datatype of categorical columns to factor

cat_cols = colnames(combined_data_filtered)
cat_cols = cat_cols[! cat_cols %in% c('idnum','cf2md_case_lib','cf2age','cm2fbir','cf2povco')]

for (col in cat_cols){
  combined_data_filtered[[col]] <- factor(combined_data_filtered[[col]])
}
table(combined_data_filtered$cf2md_case_lib)
combined_data_filtered$cf2md_case_lib <- factor(combined_data_filtered$cf2md_case_lib, levels = c(0, 1))
#table(combined_data_filtered$cf2md_case_lib)
```

## EDA:

### Target Variable:
```{r}
# Plot Father Depression Variable
filtered_cf2md_case_lib <- combined_data_filtered$cf2md_case_lib
depression_frequencies <- combined_data_filtered %>%
  group_by(cf2md_case_lib) %>%
  summarize(num_depressed = n())

ggplot(depression_frequencies, aes(x = cf2md_case_lib, y = num_depressed)) +
  geom_col() + 
  labs(title = 'Distribution of father depression frequency',
       x = 'Depression',
       y = 'Frequency') +
  theme(plot.title = element_text(hjust = 0.5))
# barplot(depression_frequencies, main = "Histogram of Father's Depression", xlab = "Depression", ylab = "Frequency", names.arg = c(0, 1), ylim = c(0, max(depression_frequencies)))
```

### Explanatory Variables:
```{r}
# Continuous variables EDA:
cont_col = c("cf2age","cm2fbir","cf2povco")

hist(combined_data_filtered$cf2age, main = "Histogram of Father's Age", xlab = "Age", breaks = 30)
hist(combined_data_filtered$cm2fbir, main = "Histogram of Mother's Age at Birth of First Child", xlab = "Age", breaks = 30)
hist(combined_data_filtered$cf2povco, main = "Ratio of Household Income to Poverty Line", xlab = "Ratio", breaks = 30) # potentially has outliers

titles <- c(cf2age = "Boxplot of Father's Age and Father's Depression", 
            cm2fbir = "Boxplot of Motherâ€™s Age at Birth of First Child and Father's Depression", 
            cf2povco = "Boxplot of Ratio of Household Income to Poverty Line")

y_labels <- c(cf2age = "Father's Age", 
              cm2fbir = "Mother's Age at First Birth", 
              cf2povco = "Income to Poverty Ratio")

for (col in cont_col) {
  title <- titles[col]
  y_label <- y_labels[col]
  p <- ggplot(combined_data_filtered, aes(x = cf2md_case_lib, y = !!sym(col))) + 
    geom_boxplot() +
    scale_x_discrete(labels = c("0" = "No Depression", "1" = "Has Depression")) +
    labs(title = title, y = y_label, x = "Depression of Father") +
    theme(plot.title = element_text(hjust = 0.5))  # Center title and angle x-axis text
  print(p)
}
  
```
### Correlation Check
```{r}
# TODO: Calculate the correlation between variables
#numeric_columns <- sapply(combined_data_filtered, is.numeric)
#combined_data_numeric <- combined_data_filtered[, numeric_columns]

#correlation_matrix <- cor(combined_data_numeric, use = "pairwise.complete.obs")
# print(correlation_matrix)
```

### Multicolinearity Analysis:
```{r}
# simple linear model
exclude_column <- c("f2b1","cf2md_case_lib") # remove because it is highly correlated with f2b1a
columns_to_use <- setdiff(names(combined_data_filtered), exclude_column)
formula <- as.formula(paste("cf2md_case_lib ~", paste(columns_to_use, collapse = " + ")))

model0 <- glm(formula, family = 'binomial', data = combined_data_filtered)
summary(model0)
```


```{r}
# Question: how do we calculate correlation between categorical features?


# VIF:

# calculate VIF:
# non of the normalized VIF are above 5 so we are good :)
vif(model0)

```
### Outlier Analysis:
There are quite a lot of outliers
```{r}
plot <- ols_plot_cooksd_bar(model0)

# Add a title and center it, and make title, x-label, y-label larger
plot <- plot +
  ggtitle("Cook's Distance Bar Plot") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 17),  # Increase title size
        axis.title.x = element_text(size = 13),  # Increase x-axis label size
        axis.title.y = element_text(size = 13))  # Increase y-axis label size

plot
```

### Check normality:
does not follow normal distribution at all T.T
```{r}
# plot qqplot:
res <- residuals(model0)
res_df = data.frame(res)

res_qq_plot <- ggplot(res_df, aes(sample = res)) +
   geom_qq() +
     geom_qq_line() +
        labs(x = "quantile",
               y = "residual")+
   ggtitle("QQ Plot of Residuals")+
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

res_qq_plot
```
### Deviance Residuals plot:
```{r}
plot(residuals(model0, type = "deviance"), main="Deviance Residuals",
     xlab="Case Number", ylab="Deviance Residuals", pch=20, col="blue",
     cex.main=1.5,    # Increases the main title size
     cex.lab=1.2,     # Increases the x and y labels size
     cex.axis=1.1)    # Optionally, increases the axis text size
abline(h = 0, col = "red")
```

```{r}
plot(model0, which = 1,
     cex.main=1.5,  # Increases the main title size
     cex.lab=1.2,   # Increases the x and y labels size
     cex.axis=1.1)  # Optionally, increases the axis text size
```

### Area Under the ROC Curve (AUC) and Confusion Matrix
```{r}
predictions <- predict(model0, type = "response")

# Calculate the AUC
roc_curve <- roc(response = combined_data_filtered$cf2md_case_lib, predictor = predictions)
auc_value <- auc(roc_curve)
print(auc_value)

# Create confusion matrix
predicted_class <- ifelse(predictions > 0.5, 1, 0)
conf_matrix <- confusionMatrix(as.factor(predicted_class), as.factor(combined_data_filtered$cf2md_case_lib))
conf_matrix

f1_score <- conf_matrix$byClass['F1']
print(paste("F1 Score:", f1_score))
```

### Penalized Logistic Regression: LASSO
```{r}
set.seed(2020)
# Applying SMOTE to balance the dataset
data_balanced <- ovun.sample(cf2md_case_lib ~ ., data = combined_data_filtered, method = "over", N = 2910*2)$data
x <- as.matrix(data_balanced[, columns_to_use])
y <- data_balanced$cf2md_case_lib

cv_lasso <- cv.glmnet(x, y, family="binomial", alpha=1)
plot(cv_lasso)
plot(cv_lasso$glmnet.fit, xvar = "lambda", label = TRUE)

#choose best lambda and refit the model
best_lambda <- cv_lasso$lambda.min
lasso_model <- glmnet(x, y, family="binomial", alpha=1, lambda=best_lambda)

# Extracting non-zero coefficients
lasso_coef <- coef(lasso_model, s = best_lambda)
print(lasso_coef)

predicted_probabilities <- predict(lasso_model, newx = x, s = best_lambda, type = "response")

# Convert probabilities to binary class predictions based on a threshold of 0.5
predicted_classes <- ifelse(predicted_probabilities > 0.5, 1, 0)

# Create confusion matrix
conf_matrix <- confusionMatrix(as.factor(predicted_classes), as.factor(y), positive = "1")
print(conf_matrix)

roc_result <- roc(y, predicted_probabilities)
auc_value <- auc(roc_result)
print(paste("Area Under the ROC Curve (AUC):", auc_value))

f1_score <- conf_matrix$byClass['F1']
print(paste("F1 Score:", f1_score))

```
### Bootstrapping the Lasso model to check stability of coefficients

```{r}
# Set the number of bootstrap replications
n_bootstraps <- 1000
coefs <- matrix(NA, ncol = length(coefficients(lasso_model)[-1]), nrow = n_bootstraps)  # prepare matrix to store coefficients

set.seed(2020)  # For reproducibility

for (i in 1:n_bootstraps) {
  # Sample with replacement from the original data
  boot_indices <- sample(1:nrow(x), replace = TRUE)
  x_boot <- x[boot_indices, ]
  y_boot <- y[boot_indices]
  
  # Fit Lasso model on the bootstrap sample
  lasso_boot <- glmnet(x_boot, y_boot, family = "binomial", alpha = 1, lambda = best_lambda)
  
  # Store the coefficients; adjust the indexing if necessary depending on whether intercept is included
  coefs[i, ] <- as.matrix(coef(lasso_boot, s = best_lambda)[-1])
}

# Calculate the mean, standard deviation and zero-count for each coefficient
coef_stats <- apply(coefs, 2, function(x) c(mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE), zero_count = sum(x == 0, na.rm = TRUE)))
coef_stats <- t(coef_stats)  # transpose for better readability

# Display the results
print(coef_stats)
```

## Advanced Model (Tree-based)
In this part, we first trained a simple decision tree model on the train split of the imbalanced dataset. We found the prediction of this model is highly biased towards the majority labels (all 0s). Based on this observation, we resampled the imbalanced dataset to create a new dataset with balanced class distribution, in order to cope with the previous class imbalance problem. Then we trained a simple decision tree model on the train split of the balanced dataset, and tested it on the test split. The model has an accuracy of 64%

### Part 1: Decision Tree Model on Imbalanced Dataset
The subset of the dataset we are using contains 2910 data points with label 0 and 336 data points with label 1, which suffers from the imbalanced class problem to some degree. If we train a vanilla tree-based model, the result model is prone to predict the majority class (in this case, all 0s).
```{r}
# split the data, train the model on the training split, and evaluate the model on the test split
training_indices <- createDataPartition(combined_data_filtered$cf2md_case_lib, p = 0.8, list = FALSE)
train_data <- combined_data_filtered[training_indices, ]
test_data <- combined_data_filtered[-training_indices, ]

tree_model0 <- rpart(formula, data = train_data, method = "class", control = rpart.control(minsplit = 10, cp = 0.1))
print(tree_model0)
```

```{r}
rpart.plot(tree_model0)
```
```{r}
# Predict and evaluate
predictions <- predict(tree_model0, test_data, type = "class")
confusionMatrix(predictions, test_data$cf2md_case_lib)
```

### Part 1: Decision Tree Model on Balanced (Resampled) Dataset
To deal with the class imbalance problem, we first perform resampling on the original data, then used the balanced, resampled data to train a decision tree model. We can see that even trained on the resampled data, the decision tree model still performs poorly, with both training and test accuracy around 64%, indicating the underfitting of simple decision tree. Hence the need for a more complex model like random forest (bagged decision trees).
```{r}
# resample the data s.t. the number of different classes equal to one another
balanced_data <- ovun.sample(cf2md_case_lib ~ ., data = combined_data_filtered, method = "over", N = 2910*2)$data
table(balanced_data$cf2md_case_lib)
```

```{r}
# perform train test split
training_indices <- createDataPartition(balanced_data$cf2md_case_lib, p = 0.8, list = FALSE)
training_indices <- training_indices[, 1]
train_data <- balanced_data[training_indices, ]
test_data <- balanced_data[-training_indices, ]
# train a decision tree model on the balanced dataset
tree_model1 <- rpart(formula, data = train_data, method = "class", control = rpart.control(minsplit = 10, cp = 0.1))
print(tree_model1)
```
```{r}
rpart.plot(tree_model1)
```
```{r}
# Predict on train data
predictions <- predict(tree_model1, train_data, type = "class")
# Evaluate model performance on the training set
confusionMatrix(predictions, train_data$cf2md_case_lib)
```

```{r}
# Predict on test data
predictions <- predict(tree_model1, test_data, type = "class")
# Evaluate model performance on the test split
confusionMatrix(predictions, test_data$cf2md_case_lib)
```

### Part 3: Random Forest Model on Balanced (Resampled) Data
```{r}
set.seed(123)  # for reproducibility
training_indices <- createDataPartition(balanced_data$cf2md_case_lib, p = 0.8, list = FALSE)
training_indices <- training_indices[, 1]
train_data <- balanced_data[training_indices, ]
test_data <- balanced_data[-training_indices, ]
# Train the model
rf_model <- randomForest(formula, data = train_data, ntree = 500, mtry = sqrt(ncol(train_data) - 1), importance = TRUE)

# Print the model summary
print(rf_model)
```
```{r}
# Predict on the test data
predictions <- predict(rf_model, test_data)
# Evaluate the predictions on the test dataset
confusionMatrix(predictions, test_data$cf2md_case_lib)
```
```{r}
# Check variable importance
importance(rf_model)
varImpPlot(rf_model)
```
#### Interpretation of the Random Forest Model
From the variance importance plot of the random forest model, we can see that the top 5 most important covariate are, in descending order, `cf2povco`, `cf2age`, `cm2fbir`, `m2c33`, and `cm2edu`, according to the mean decrease accuracy and the mean decreased gini.


#### Poster Plots:

Target Variable:
```{r}
# Plot Father Depression Variable
filtered_cf2md_case_lib <- combined_data_filtered$cf2md_case_lib
depression_frequencies <- combined_data_filtered %>%
  group_by(cf2md_case_lib) %>%
  summarize(num_depressed = n())

colors = c('#6baed6','#08519c')

ggplot(depression_frequencies, 
       aes(x = cf2md_case_lib, y = num_depressed, fill = cf2md_case_lib)) +
  geom_col() + 
  labs(title = "Distribution of Father's Depression",
       x = 'Depression',
       y = 'Frequency') +
  scale_x_discrete(labels = c("0" = "No Depression", "1" = "Has Depression")) +
  scale_fill_manual(values = colors) +
  theme_minimal() +
  theme(
    panel.grid.minor = element_blank(), 
    panel.grid.major.x = element_blank(),
    plot.title = element_text(hjust = 0.5),
    legend.position = "none")
```

```{r}
# plot box plot of age vs depression
ggplot(combined_data_filtered, aes(x = cf2md_case_lib, 
                                          y = cf2age, 
                                          fill = cf2md_case_lib)) + 
    geom_boxplot() +
    scale_x_discrete(labels = c("0" = "No Depression", "1" = "Has Depression")) +
    scale_fill_manual(values = colors) +
    #scale_color_manual(values = c("#bdbdbd","#969696")) + 
    theme_minimal() +
    theme(
      panel.grid.minor = element_blank(), 
      panel.grid.major.x = element_blank(),
      plot.title = element_text(hjust = 0.5),
      legend.position = "none") + 
  labs(title = "Boxplot of Father's Age vs. Depression",
        x = "Depression",
       y = "Father's Age")
```

```{r}
# plot histogram of log of income to poverty threshold ratio
combined_data_filtered <- mutate(combined_data_filtered, log_cf2povco = log(cf2povco))

ggplot(combined_data_filtered, aes(x = log_cf2povco)) + 
    geom_histogram(bins = 20,  fill = "#08519c") +
    labs(title = "Distribution of Househole Income to Poverty Threshold Ratio", 
         y = "Frequency",
         x = "log(Househole Income to Poverty Threshold Ratio)") +
   theme_minimal() +
    theme(
      panel.grid.minor = element_blank(), 
      panel.grid.major.x = element_blank(),
      plot.title = element_text(hjust = 0.5)) # Center title and angle x-axis text
```

```{r}
# plot distribution of race:

race_levels <- c(-1,1,2,3,4,5)
race_labels <- c("Missing", "White", "Blck", "Asian", "AmInd", "Other") 
combined_data_filtered$f1h3_factor <- factor(combined_data_filtered$f1h3, levels = race_levels, labels = race_labels)

combined_face_count <- combined_data_filtered %>%
  group_by(f1h3_factor) %>%
  summarize(race_count = n())

ggplot(combined_face_count, aes(x = reorder(f1h3_factor, race_count), y = race_count, fill = reorder(f1h3_factor, race_count))) +
  geom_col() + 
  labs(title = "Distribution of Father's Race", 
         x = "Race",
         y = "Frequency") +
  scale_fill_manual(values = c('#9ecae1','#6baed6','#4292c6','#2171b5','#08519c','#08306b')) +
   theme_minimal() +
    theme(
      panel.grid.minor = element_blank(), 
      panel.grid.major.x = element_blank(),
      plot.title = element_text(hjust = 0.5),
      legend.position = "none"
      ) # Center title and angle x-axis text
```

